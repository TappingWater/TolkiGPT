{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de60d78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanaka/miniconda3/envs/tolkigpt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from fastcoref import FCoref\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import os\n",
    "import torch # For device check\n",
    "from collections import defaultdict\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c905863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet data found.\n",
      "Loading spaCy model: en_core_web_lg\n",
      "\n",
      "Initializing FastCoref model...\n",
      "Attempting to use MPS (Apple Silicon GPU).\n",
      "MPS initialization failed (FCoref.__init__() got multiple values for argument 'device'). Falling back to CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/21/2025 16:45:56 - INFO - \t missing_keys: []\n",
      "04/21/2025 16:45:56 - INFO - \t unexpected_keys: []\n",
      "04/21/2025 16:45:56 - INFO - \t mismatched_keys: []\n",
      "04/21/2025 16:45:56 - INFO - \t error_msgs: []\n",
      "04/21/2025 16:45:56 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/21/2025 16:45:56 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized FastCoref on CPU.\n",
      "\n",
      "--- Processing Text ---\n",
      "Apple Inc., founded by Steve Jobs and Steve Wozniak, is based in Cupertino.\n",
      "    Tim Cook became the CEO of Apple in 2011. He previously worked at IBM.\n",
      "    Apple produces the popular iPhone. Google, its competitor, makes Android.\n",
      "    Steve Jobs also co-founded Pixar, which was later acquired by Disney. Bob Iger leads Disney.\n",
      "\n",
      "1. Running spaCy NLP pipeline...\n",
      "\n",
      "2. Extracting initial NER entities...\n",
      "   Initial entities: {'Apple Inc.': 'ORG', 'Steve Jobs': 'PERSON', 'Steve Wozniak': 'PERSON', 'Cupertino': 'GPE', 'Tim Cook': 'PERSON', 'Apple': 'ORG', '2011': 'DATE', 'IBM': 'ORG', 'iPhone': 'ORG', 'Google': 'ORG', 'Android': 'ORG', 'Pixar': 'ORG', 'Disney': 'ORG', 'Bob Iger': 'PERSON'}\n",
      "\n",
      "3. Running FastCoref pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 40.70 examples/s]\n",
      "04/21/2025 16:45:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Coref Clusters (Indices): [[(0, 52), (107, 112), (155, 160), (198, 201)], [(80, 88), (122, 124)], [(23, 33), (233, 243)], [(295, 301), (318, 324)]]\n",
      "   Coref Clusters (Strings): [['Apple Inc., founded by Steve Jobs and Steve Wozniak,', 'Apple', 'Apple', 'its'], ['Tim Cook', 'He'], ['Steve Jobs', 'Steve Jobs'], ['Disney', 'Disney']]\n",
      "\n",
      "4. Building initial coreference map...\n",
      "   Initial Coref Map: {0: 'Apple Inc., founded by Steve Jobs and Steve Wozniak,', 107: 'Apple Inc., founded by Steve Jobs and Steve Wozniak,', 155: 'Apple Inc., founded by Steve Jobs and Steve Wozniak,', 198: 'Apple Inc., founded by Steve Jobs and Steve Wozniak,', 80: 'Tim Cook', 122: 'Tim Cook', 23: 'Steve Jobs', 233: 'Steve Jobs', 295: 'Disney', 318: 'Disney'}\n",
      "\n",
      "5. Refining coreference map...\n",
      "   Refined Coref Map: {0: 'Apple', 107: 'Apple', 155: 'Apple', 198: 'Apple', 80: 'Tim Cook', 122: 'Tim Cook', 23: 'Steve Jobs', 233: 'Steve Jobs', 295: 'Disney', 318: 'Disney'}\n",
      "\n",
      "6. Extracting enhanced relationships...\n",
      "  Extracted Rel: (Apple) --[ESTABLISH_IN]--> (Cupertino) | Attrs: {}\n",
      "  Extracted Rel: (Tim Cook) --[WORK_AT]--> (IBM) | Attrs: {'manner': 'previously'}\n",
      "  Extracted Rel: (Apple) --[PRODUCE]--> (iPhone) | Attrs: {}\n",
      "  Extracted Rel: (Google) --[MAKE]--> (Android) | Attrs: {}\n",
      "  Extracted Rel: (Disney) --[GET]--> (Steve Jobs) | Attrs: {'manner': 'later'}\n",
      "  Extracted Rel: (Bob Iger) --[LEAD]--> (Disney) | Attrs: {}\n",
      "\n",
      "7. Extracting final entities...\n",
      "\n",
      "Final Entities (NER + Refined Coref):\n",
      "  - Original: 'Apple Inc.' (ORG), StartChar: 0 -> Resolved: 'Apple'\n",
      "  - Original: 'Steve Jobs' (PERSON), StartChar: 23 -> Resolved: 'Steve Jobs'\n",
      "  - Original: 'Steve Wozniak' (PERSON), StartChar: 38 -> Resolved: 'Steve Wozniak'\n",
      "  - Original: 'Cupertino' (GPE), StartChar: 65 -> Resolved: 'Cupertino'\n",
      "  - Original: 'Tim Cook' (PERSON), StartChar: 80 -> Resolved: 'Tim Cook'\n",
      "  - Original: 'Apple' (ORG), StartChar: 107 -> Resolved: 'Apple'\n",
      "  - Original: '2011' (DATE), StartChar: 116 -> Resolved: '2011'\n",
      "  - Original: 'IBM' (ORG), StartChar: 146 -> Resolved: 'IBM'\n",
      "  - Original: 'Apple' (ORG), StartChar: 155 -> Resolved: 'Apple'\n",
      "  - Original: 'iPhone' (ORG), StartChar: 182 -> Resolved: 'iPhone'\n",
      "  - Original: 'Google' (ORG), StartChar: 190 -> Resolved: 'Google'\n",
      "  - Original: 'Android' (ORG), StartChar: 220 -> Resolved: 'Android'\n",
      "  - Original: 'Steve Jobs' (PERSON), StartChar: 233 -> Resolved: 'Steve Jobs'\n",
      "  - Original: 'Pixar' (ORG), StartChar: 260 -> Resolved: 'Pixar'\n",
      "  - Original: 'Disney' (ORG), StartChar: 295 -> Resolved: 'Disney'\n",
      "  - Original: 'Bob Iger' (PERSON), StartChar: 303 -> Resolved: 'Bob Iger'\n",
      "  - Original: 'Disney' (ORG), StartChar: 318 -> Resolved: 'Disney'\n",
      "\n",
      "--- Processing Complete ---\n",
      "\n",
      "============================== FINAL RESULTS ==============================\n",
      "\n",
      "Entities (Resolved Name: Type):\n",
      "  - 2011: DATE\n",
      "  - Android: ORG\n",
      "  - Apple: ORG\n",
      "  - Bob Iger: PERSON\n",
      "  - Cupertino: GPE\n",
      "  - Disney: ORG\n",
      "  - Google: ORG\n",
      "  - IBM: ORG\n",
      "  - Pixar: ORG\n",
      "  - Steve Jobs: PERSON\n",
      "  - Steve Wozniak: PERSON\n",
      "  - Tim Cook: PERSON\n",
      "  - iPhone: ORG\n",
      "\n",
      "Relationships (Subject, Verb, Object, Attributes):\n",
      "  - (Apple) --[ESTABLISH_IN]--> (Cupertino) \n",
      "  - (Apple) --[PRODUCE]--> (iPhone) \n",
      "  - (Bob Iger) --[LEAD]--> (Disney) \n",
      "  - (Disney) --[GET]--> (Steve Jobs) | {'manner': 'later'}\n",
      "  - (Google) --[MAKE]--> (Android) \n",
      "  - (Tim Cook) --[WORK_AT]--> (IBM) | {'manner': 'previously'}\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "spacy_model = \"en_core_web_lg\"\n",
    "# Try the potentially more accurate LingMess model\n",
    "fastcoref_model_name = \"biu-nlp/f-coref\"\n",
    "# fastcoref_model_name = 'biu-nlp/f-coref' # Alternative faster model\n",
    "\n",
    "# --- Download NLTK WordNet Data (if needed) ---\n",
    "try:\n",
    "    wordnet.synsets('computer')\n",
    "    print(\"WordNet data found.\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK WordNet data...\")\n",
    "    nltk.download('wordnet')\n",
    "    print(\"WordNet download complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during WordNet check/download: {e}\")\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_verb_synset_lemma(verb_token):\n",
    "    \"\"\"Gets the first lemma of the first WordNet synset for a verb token.\"\"\"\n",
    "    if not verb_token or verb_token.pos_ != \"VERB\":\n",
    "        return None\n",
    "    verb_lemma = verb_token.lemma_\n",
    "    try:\n",
    "        synsets = wordnet.synsets(verb_lemma, pos=wordnet.VERB)\n",
    "        if synsets:\n",
    "            # Use the first lemma of the most common sense\n",
    "            normalized = synsets[0].lemmas()[0].name()\n",
    "            return normalized.replace('_', '').upper() # Use uppercase for convention\n",
    "        else:\n",
    "            # Fallback to the original lemma if not found\n",
    "            return verb_lemma.upper()\n",
    "    except Exception as e:\n",
    "        print(f\"WARN: Error normalizing verb '{verb_lemma}' with WordNet: {e}\")\n",
    "        return verb_lemma.upper() # Fallback on error\n",
    "\n",
    "def build_coref_map(text, clusters_indices):\n",
    "    \"\"\"\n",
    "    Builds an initial map from the start character index of a mention\n",
    "    to the raw text of the representative mention in its cluster.\n",
    "    \"\"\"\n",
    "    char_coref_mapping = {}\n",
    "    if not clusters_indices:\n",
    "        return char_coref_mapping\n",
    "\n",
    "    for cluster in clusters_indices:\n",
    "        if not cluster: continue\n",
    "        rep_start_char, rep_end_char = cluster[0]\n",
    "        # Store the raw text initially\n",
    "        rep_text = text[rep_start_char:rep_end_char].strip()\n",
    "        for mention_start_char, mention_end_char in cluster:\n",
    "            char_coref_mapping[mention_start_char] = rep_text\n",
    "    return char_coref_mapping\n",
    "\n",
    "def refine_coref_map_with_entities(initial_char_coref_map, initial_entities):\n",
    "    \"\"\"\n",
    "    Refines the coref map by replacing long/descriptive representative mentions\n",
    "    with shorter NER entity names where possible.\n",
    "    initial_entities: dict {entity_text: label} from initial NER pass\n",
    "    \"\"\"\n",
    "    refined_map = initial_char_coref_map.copy()\n",
    "    # Prefer shorter, known entity names\n",
    "    entity_names = sorted(list(initial_entities.keys()), key=len) # Sort by length ascending\n",
    "\n",
    "    long_name_to_short = {}\n",
    "\n",
    "    # Find potential short names for long resolved names currently in the map values\n",
    "    current_rep_texts = set(initial_char_coref_map.values())\n",
    "    for long_name in current_rep_texts:\n",
    "         # Heuristic: check if name seems too long/descriptive or isn't in initial entities\n",
    "        if (',' in long_name or len(long_name) > 30) or (long_name not in initial_entities):\n",
    "            best_match = None\n",
    "            # Find the shortest entity name that's part of the long name\n",
    "            for short_name in entity_names:\n",
    "                # Check containment, ensure it's a meaningful part (optional enhancement)\n",
    "                if short_name in long_name and len(short_name) >= 3:\n",
    "                    best_match = short_name # Found shortest NER name within long name\n",
    "                    break # Take the first (shortest) match\n",
    "            if best_match:\n",
    "                long_name_to_short[long_name] = best_match\n",
    "\n",
    "    # Create the new map using the refined names\n",
    "    for char_idx, mapped_text in initial_char_coref_map.items():\n",
    "        # If the current mapped text has a shorter version identified, use it\n",
    "        refined_map[char_idx] = long_name_to_short.get(mapped_text, mapped_text)\n",
    "\n",
    "    return refined_map\n",
    "\n",
    "def get_span_resolved_text(span, refined_char_coref_map):\n",
    "    \"\"\"\n",
    "    Resolves the text of a spaCy Span using the refined coref map,\n",
    "    checking the span's start character index. Falls back to span text.\n",
    "    \"\"\"\n",
    "    if not span: return None\n",
    "    return refined_char_coref_map.get(span.start_char, span.text.strip())\n",
    "\n",
    "def get_token_resolved_text(token, refined_char_coref_map):\n",
    "     \"\"\"\n",
    "     Resolves the text for a single token - less preferred than resolving spans.\n",
    "     \"\"\"\n",
    "     if not token: return None\n",
    "     return refined_char_coref_map.get(token.idx, token.text.strip())\n",
    "\n",
    "def get_entity_span_for_token(token, doc):\n",
    "    \"\"\"Finds the encompassing NER entity span for a token, if any.\"\"\"\n",
    "    for ent in doc.ents:\n",
    "        if token.i >= ent.start and token.i < ent.end:\n",
    "            return ent\n",
    "    return None\n",
    "\n",
    "def extract_relation_attributes(token):\n",
    "    \"\"\"\n",
    "    Extracts attributes (time, location, manner) linked to a verb/action token\n",
    "    by checking its children in the dependency tree.\n",
    "    \"\"\"\n",
    "    attributes = {}\n",
    "    for child in token.children:\n",
    "        # Time: Look for prepositions like \"in\", \"on\", \"at\" governing DATE entities\n",
    "        if child.dep_ == 'prep' and child.text.lower() in ['in', 'on', 'at', 'during', 'since', 'until']:\n",
    "            for grandchild in child.children:\n",
    "                if grandchild.dep_ == 'pobj':\n",
    "                     ent_span = get_entity_span_for_token(grandchild, grandchild.doc)\n",
    "                     if ent_span and ent_span.label_ == 'DATE':\n",
    "                         attributes['time'] = ent_span.text.strip()\n",
    "                         break # Found time\n",
    "                     elif grandchild.like_num: # Handle simple numbers like years\n",
    "                          attributes['time'] = grandchild.text.strip()\n",
    "                          break\n",
    "\n",
    "        # Location: Look for prepositions governing GPE/LOC entities\n",
    "        elif child.dep_ == 'prep' and child.text.lower() in ['in', 'at', 'on', 'near', 'from', 'to']:\n",
    "             for grandchild in child.children:\n",
    "                 if grandchild.dep_ == 'pobj':\n",
    "                    ent_span = get_entity_span_for_token(grandchild, grandchild.doc)\n",
    "                    if ent_span and ent_span.label_ in ['GPE', 'LOC', 'FAC']:\n",
    "                        attributes['location'] = ent_span.text.strip()\n",
    "                        break # Found location\n",
    "\n",
    "        # Manner: Look for adverbial modifiers\n",
    "        elif child.dep_ == 'advmod':\n",
    "            attributes.setdefault('manner', []).append(child.text.strip())\n",
    "\n",
    "    # Combine manner adverbs if multiple found\n",
    "    if 'manner' in attributes:\n",
    "        attributes['manner'] = \" \".join(attributes['manner'])\n",
    "\n",
    "    return attributes\n",
    "\n",
    "def extract_enhanced_relationships(doc, refined_char_coref_map):\n",
    "    \"\"\"\n",
    "    Extracts relationships with improved entity span resolution,\n",
    "    multi-word relation extraction, and attribute extraction.\n",
    "    \"\"\"\n",
    "    relationships = []\n",
    "    processed_verbs = set() # Avoid processing compound verbs multiple times\n",
    "\n",
    "    for token in doc:\n",
    "        if token.i in processed_verbs: continue\n",
    "\n",
    "        relation_info = None\n",
    "\n",
    "        # --- Trigger based on Verbs ---\n",
    "        if token.pos_ == \"VERB\":\n",
    "            # Handle auxiliary verbs and passive voice\n",
    "            subject_token = None\n",
    "            object_token = None\n",
    "            passive = False\n",
    "            verb_phrase_tokens = [token] # Start with current verb\n",
    "\n",
    "            # Find subject (nsubj or nsubjpass)\n",
    "            for child in token.children:\n",
    "                if \"subj\" in child.dep_:\n",
    "                    subject_token = child\n",
    "                    if child.dep_ == \"nsubjpass\":\n",
    "                        passive = True\n",
    "                    break # Assume one subject per verb for simplicity here\n",
    "\n",
    "            # If no subject attached to this verb, check its head (auxiliary case)\n",
    "            if not subject_token and token.dep_ in [\"aux\", \"auxpass\", \"xcomp\", \"ccomp\", \"advcl\"]:\n",
    "                 verb_head = token.head\n",
    "                 if verb_head.pos_ == \"VERB\":\n",
    "                     # Mark related verb tokens to avoid re-processing\n",
    "                     processed_verbs.add(verb_head.i)\n",
    "                     verb_phrase_tokens.insert(0, verb_head) # Add head verb\n",
    "                     # Find subject attached to the head verb\n",
    "                     for child in verb_head.children:\n",
    "                         if \"subj\" in child.dep_:\n",
    "                            subject_token = child\n",
    "                            if child.dep_ == \"nsubjpass\": passive = True\n",
    "                            break\n",
    "\n",
    "            if not subject_token: continue # Cannot form relationship without subject\n",
    "\n",
    "            # Determine the main verb for relation name and attributes\n",
    "            main_verb_token = token if token.dep_ not in [\"aux\", \"auxpass\"] else token.head\n",
    "            if main_verb_token.pos_ != \"VERB\": continue # Ensure it's still a verb\n",
    "\n",
    "            # Find object(s) - dobj, pobj (via prep), attr, oprd, agent (passive)\n",
    "            prep_text = None\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"dobj\":\n",
    "                    object_token = child\n",
    "                    break\n",
    "                elif child.dep_ == \"attr\": # Attribute link (e.g., \"is CEO\")\n",
    "                    object_token = child\n",
    "                    break\n",
    "                elif child.dep_ == \"oprd\": # Object predicate\n",
    "                     object_token = child\n",
    "                     break\n",
    "                elif child.dep_ == \"prep\": # Prepositional object\n",
    "                    prep_text = child.text.lower()\n",
    "                    for grandchild in child.children:\n",
    "                        if grandchild.dep_ == \"pobj\":\n",
    "                             object_token = grandchild\n",
    "                             break # Take first pobj\n",
    "                    if object_token: break\n",
    "                elif passive and child.dep_ == \"agent\": # Agent in passive voice (\"by X\")\n",
    "                    for grandchild in child.children:\n",
    "                        if grandchild.dep_ == \"pobj\":\n",
    "                            # In passive, the agent becomes the logical subject\n",
    "                            object_token = subject_token # Original subject is logical object\n",
    "                            subject_token = grandchild # Agent is logical subject\n",
    "                            break\n",
    "                    if subject_token: break\n",
    "\n",
    "\n",
    "            if not object_token: continue # Require subject and object\n",
    "\n",
    "            # --- Resolve Entities for the full spans ---\n",
    "            subj_span = get_entity_span_for_token(subject_token, doc) or subject_token.sent # Fallback to sentence span? No, use token/chunk\n",
    "            if not subj_span: subj_span = next(subject_token.subtree, subject_token) # Basic chunk idea\n",
    "\n",
    "            obj_span = get_entity_span_for_token(object_token, doc) or object_token.sent\n",
    "            if not obj_span: obj_span = next(object_token.subtree, object_token) # Basic chunk idea\n",
    "\n",
    "            resolved_subj = get_span_resolved_text(subj_span if isinstance(subj_span, spacy.tokens.Span) else None, refined_char_coref_map) or get_token_resolved_text(subject_token, refined_char_coref_map)\n",
    "            resolved_obj = get_span_resolved_text(obj_span if isinstance(obj_span, spacy.tokens.Span) else None, refined_char_coref_map) or get_token_resolved_text(object_token, refined_char_coref_map)\n",
    "\n",
    "\n",
    "            # --- Determine Relation Phrase ---\n",
    "            relation_phrase = get_verb_synset_lemma(main_verb_token) or main_verb_token.lemma_.upper()\n",
    "            # Enhance with prepositions or particles if present\n",
    "            if prep_text:\n",
    "                relation_phrase = f\"{relation_phrase}_{prep_text.upper()}\"\n",
    "            else:\n",
    "                 for child in main_verb_token.children:\n",
    "                      if child.dep_ == 'prt': # Particle, e.g., \"set up\"\n",
    "                           relation_phrase = f\"{relation_phrase}_{child.text.upper()}\"\n",
    "                           break\n",
    "\n",
    "\n",
    "            # --- Extract Attributes ---\n",
    "            attributes = extract_relation_attributes(main_verb_token)\n",
    "\n",
    "            # Add the relationship\n",
    "            if resolved_subj and resolved_obj and resolved_subj.lower() != resolved_obj.lower():\n",
    "                relation_info = (resolved_subj, relation_phrase, resolved_obj, attributes)\n",
    "                print(f\"  Extracted Rel: ({resolved_subj}) --[{relation_phrase}]--> ({resolved_obj}) | Attrs: {attributes}\")\n",
    "                relationships.append(relation_info)\n",
    "\n",
    "    # Add rules for specific verbs like 'founded', 'co-founded', 'acquired' if needed\n",
    "\n",
    "    return relationships\n",
    "\n",
    "\n",
    "def extract_final_entities(doc, refined_char_coref_map):\n",
    "    \"\"\"\n",
    "    Extracts Named Entities and resolves their names using the *refined* coref map.\n",
    "    Returns dict: {resolved_entity_name: entity_label}\n",
    "    \"\"\"\n",
    "    entities = {}\n",
    "    print(\"\\nFinal Entities (NER + Refined Coref):\")\n",
    "    for ent in doc.ents:\n",
    "        resolved_name = refined_char_coref_map.get(ent.start_char, ent.text.strip())\n",
    "        label = ent.label_\n",
    "        print(f\"  - Original: '{ent.text}' ({label}), StartChar: {ent.start_char} -> Resolved: '{resolved_name}'\")\n",
    "\n",
    "        if resolved_name not in entities:\n",
    "            entities[resolved_name] = label\n",
    "        elif entities[resolved_name] != label:\n",
    "             print(f\"    WARN: Conflicting labels for '{resolved_name}'. Keeping '{entities[resolved_name]}', ignoring new label '{label}'.\")\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "# --- Main Pipeline Function ---\n",
    "\n",
    "def process_text_to_graph_info_v2(text, spacy_nlp, fastcoref_model):\n",
    "    \"\"\"\n",
    "    Revised pipeline implementing improvements.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing Text ---\")\n",
    "    print(text.strip()) # Use stripped text\n",
    "\n",
    "    # 1. Process with spaCy\n",
    "    print(\"\\n1. Running spaCy NLP pipeline...\")\n",
    "    doc = spacy_nlp(text.strip()) # Process stripped text\n",
    "\n",
    "    # 2. Initial NER Extraction (for refining coref map)\n",
    "    print(\"\\n2. Extracting initial NER entities...\")\n",
    "    initial_entities = {ent.text.strip(): ent.label_ for ent in doc.ents}\n",
    "    print(f\"   Initial entities: {initial_entities}\")\n",
    "\n",
    "    # 3. Run Coreference Resolution\n",
    "    print(\"\\n3. Running FastCoref pipeline...\")\n",
    "    preds = fastcoref_model.predict(texts=[text.strip()])\n",
    "    result = preds[0]\n",
    "    clusters_indices = result.get_clusters(as_strings=False)\n",
    "    clusters_strings = result.get_clusters(as_strings=True) # For logging\n",
    "    print(f\"   Coref Clusters (Indices): {clusters_indices}\")\n",
    "    print(f\"   Coref Clusters (Strings): {clusters_strings}\")\n",
    "\n",
    "    # 4. Build Initial Coref Map\n",
    "    print(\"\\n4. Building initial coreference map...\")\n",
    "    initial_char_coref_map = build_coref_map(text.strip(), clusters_indices)\n",
    "    print(f\"   Initial Coref Map: {initial_char_coref_map}\")\n",
    "\n",
    "    # 5. Refine Coref Map\n",
    "    print(\"\\n5. Refining coreference map...\")\n",
    "    refined_char_coref_map = refine_coref_map_with_entities(initial_char_coref_map, initial_entities)\n",
    "    print(f\"   Refined Coref Map: {refined_char_coref_map}\")\n",
    "\n",
    "    # 6. Extract Enhanced Relationships\n",
    "    print(\"\\n6. Extracting enhanced relationships...\")\n",
    "    relationships = extract_enhanced_relationships(doc, refined_char_coref_map)\n",
    "\n",
    "    # 7. Extract Final Entities using Refined Map\n",
    "    print(\"\\n7. Extracting final entities...\")\n",
    "    final_entities = extract_final_entities(doc, refined_char_coref_map)\n",
    "\n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    return relationships, final_entities\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 1. Load spaCy Model ---\n",
    "    print(f\"Loading spaCy model: {spacy_model}\")\n",
    "    try:\n",
    "        nlp = spacy.load(spacy_model)\n",
    "    except OSError:\n",
    "        print(f\"Error loading spaCy model '{spacy_model}'. Download it.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize FastCoref Model ---\n",
    "    print(\"\\nInitializing FastCoref model...\")\n",
    "    # Determine device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(\"Attempting to use MPS (Apple Silicon GPU).\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "        print(\"Attempting to use CUDA GPU.\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"GPU not available, using CPU.\")\n",
    "\n",
    "    try:\n",
    "        # Try initializing with the determined device\n",
    "        fc_model = FCoref(fastcoref_model_name, nlp, device=device)\n",
    "        print(f\"Successfully initialized FastCoref on {device.upper()}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"{device.upper()} initialization failed ({e}). Falling back to CPU.\")\n",
    "        # Fallback to CPU if GPU fails\n",
    "        device = 'mps'\n",
    "        fc_model = FCoref(fastcoref_model_name, nlp=nlp, device=device)\n",
    "        print(\"Successfully initialized FastCoref on CPU.\")\n",
    "\n",
    "\n",
    "    # --- 3. Define Input Text ---\n",
    "    text = \"\"\"\n",
    "    Apple Inc., founded by Steve Jobs and Steve Wozniak, is based in Cupertino.\n",
    "    Tim Cook became the CEO of Apple in 2011. He previously worked at IBM.\n",
    "    Apple produces the popular iPhone. Google, its competitor, makes Android.\n",
    "    Steve Jobs also co-founded Pixar, which was later acquired by Disney. Bob Iger leads Disney.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 4. Run the Full Pipeline ---\n",
    "    extracted_relationships, extracted_entities = process_text_to_graph_info_v2(\n",
    "        text, nlp, fc_model\n",
    "    )\n",
    "\n",
    "    # --- 5. Display Results ---\n",
    "    print(\"\\n\" + \"=\"*30 + \" FINAL RESULTS \" + \"=\"*30)\n",
    "    print(\"\\nEntities (Resolved Name: Type):\")\n",
    "    if extracted_entities:\n",
    "        # Sort for consistent output\n",
    "        for name in sorted(extracted_entities.keys()):\n",
    "            print(f\"  - {name}: {extracted_entities[name]}\")\n",
    "    else:\n",
    "        print(\"  (No entities found)\")\n",
    "\n",
    "    print(\"\\nRelationships (Subject, Verb, Object, Attributes):\")\n",
    "    if extracted_relationships:\n",
    "        # Sort for consistent output\n",
    "        extracted_relationships.sort()\n",
    "        for rel in extracted_relationships:\n",
    "            subj, verb, obj, attrs = rel\n",
    "            attr_str = f\"| {attrs}\" if attrs else \"\"\n",
    "            print(f\"  - ({subj}) --[{verb}]--> ({obj}) {attr_str}\")\n",
    "    else:\n",
    "        print(\"  (No relationships found)\")\n",
    "\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb7be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import spacy\n",
    "\n",
    "# 1. Neo4j Connection Details\n",
    "uri = \"bolt://localhost:7687\"  # Default Bolt port\n",
    "username = \"neo4j\"  # Or your username\n",
    "password = \"thesith123\"  # Replace with the password you set\n",
    "\n",
    "# 2. Initialize Neo4j Driver\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "# 3. SpaCy Setup\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed08849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and retrieved a test node from Neo4j!\n"
     ]
    }
   ],
   "source": [
    "def test_connection(uri, username, password):\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "    result = None\n",
    "    try:\n",
    "        def create_test_node(tx):\n",
    "            query = \"CREATE (test:TestNode {message: 'Hello from Python'}) RETURN test.message AS message\"\n",
    "            result = tx.run(query).single()\n",
    "            return result['message'] if result else None\n",
    "\n",
    "        with driver.session() as session:\n",
    "            message = session.execute_write(create_test_node)\n",
    "            if message == 'Hello from Python':\n",
    "                print(\"Successfully created and retrieved a test node from Neo4j!\")\n",
    "            else:\n",
    "                print(\"Failed to retrieve the expected message.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_connection(uri, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b01862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model: en_core_web_lg\n",
      "\n",
      "Initializing FastCoref model...\n",
      "Attempting to use MPS (Apple Silicon GPU).\n",
      "MPS initialization failed (FCoref.__init__() got multiple values for argument 'device'). Falling back to CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/21/2025 16:45:59 - INFO - \t missing_keys: []\n",
      "04/21/2025 16:45:59 - INFO - \t unexpected_keys: []\n",
      "04/21/2025 16:45:59 - INFO - \t mismatched_keys: []\n",
      "04/21/2025 16:45:59 - INFO - \t error_msgs: []\n",
      "04/21/2025 16:45:59 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/21/2025 16:46:00 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized FastCoref on CPU.\n",
      "\n",
      "--- Processing Text ---\n",
      "Apple Inc., founded by Steve Jobs and Steve Wozniak, is based in Cupertino.\n",
      "    Tim Cook became the CEO of Apple in 2011. He previously worked at IBM.\n",
      "    Apple produces the popular iPhone. Google, its competitor, makes Android.\n",
      "    Steve Jobs also co-founded Pixar, which was later acquired by Disney. Bob Iger leads Disney.\n",
      "\n",
      "1. Running spaCy NLP pipeline...\n",
      "\n",
      "2. Extracting initial NER entities...\n",
      "   Initial entities: {'Apple Inc.': 'ORG', 'Steve Jobs': 'PERSON', 'Steve Wozniak': 'PERSON', 'Cupertino': 'GPE', 'Tim Cook': 'PERSON', 'Apple': 'ORG', '2011': 'DATE', 'IBM': 'ORG', 'iPhone': 'ORG', 'Google': 'ORG', 'Android': 'ORG', 'Pixar': 'ORG', 'Disney': 'ORG', 'Bob Iger': 'PERSON'}\n",
      "\n",
      "3. Running FastCoref pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 98.59 examples/s]\n",
      "04/21/2025 16:46:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.98it/s]\n",
      "/var/folders/kx/knyxy92s5hd9dyvldh8tggtm0000gn/T/ipykernel_262/95901110.py:29: DeprecationWarning: `id` is deprecated, use `element_id` instead\n",
      "  print(f\"Stored node: ID={node_object.id}, Labels={list(node_object.labels)}, Properties={dict(node_object)}\")\n",
      "/var/folders/kx/knyxy92s5hd9dyvldh8tggtm0000gn/T/ipykernel_262/95901110.py:59: DeprecationWarning: `id` is deprecated, use `element_id` instead\n",
      "  print(f\"Executing query: MATCH (a) WHERE id(a)={subj_node.id} MATCH (b) WHERE id(b)={obj_node.id} ...\")\n",
      "/var/folders/kx/knyxy92s5hd9dyvldh8tggtm0000gn/T/ipykernel_262/95901110.py:63: DeprecationWarning: `id` is deprecated, use `element_id` instead\n",
      "  subject_id=subj_node.id, # Pass the internal ID\n",
      "/var/folders/kx/knyxy92s5hd9dyvldh8tggtm0000gn/T/ipykernel_262/95901110.py:64: DeprecationWarning: `id` is deprecated, use `element_id` instead\n",
      "  object_id=obj_node.id,   # Pass the internal ID\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 2, column: 33, offset: 33} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:ESTABLISH_IN]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 3, column: 33, offset: 85} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:ESTABLISH_IN]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 2, column: 33, offset: 33} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:PRODUCE]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 3, column: 33, offset: 85} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:PRODUCE]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Coref Clusters (Indices): [[(0, 52), (107, 112), (155, 160), (198, 201)], [(80, 88), (122, 124)], [(23, 33), (233, 243)], [(295, 301), (318, 324)]]\n",
      "   Coref Clusters (Strings): [['Apple Inc., founded by Steve Jobs and Steve Wozniak,', 'Apple', 'Apple', 'its'], ['Tim Cook', 'He'], ['Steve Jobs', 'Steve Jobs'], ['Disney', 'Disney']]\n",
      "\n",
      "4. Building initial coreference map...\n",
      "   Initial Coref Map: {0: 'Apple Inc., founded by Steve Jobs and Steve Wozniak,', 107: 'Apple Inc., founded by Steve Jobs and Steve Wozniak,', 155: 'Apple Inc., founded by Steve Jobs and Steve Wozniak,', 198: 'Apple Inc., founded by Steve Jobs and Steve Wozniak,', 80: 'Tim Cook', 122: 'Tim Cook', 23: 'Steve Jobs', 233: 'Steve Jobs', 295: 'Disney', 318: 'Disney'}\n",
      "\n",
      "5. Refining coreference map...\n",
      "   Refined Coref Map: {0: 'Apple', 107: 'Apple', 155: 'Apple', 198: 'Apple', 80: 'Tim Cook', 122: 'Tim Cook', 23: 'Steve Jobs', 233: 'Steve Jobs', 295: 'Disney', 318: 'Disney'}\n",
      "\n",
      "6. Extracting enhanced relationships...\n",
      "  Extracted Rel: (Apple) --[ESTABLISH_IN]--> (Cupertino) | Attrs: {}\n",
      "  Extracted Rel: (Tim Cook) --[WORK_AT]--> (IBM) | Attrs: {'manner': 'previously'}\n",
      "  Extracted Rel: (Apple) --[PRODUCE]--> (iPhone) | Attrs: {}\n",
      "  Extracted Rel: (Google) --[MAKE]--> (Android) | Attrs: {}\n",
      "  Extracted Rel: (Disney) --[GET]--> (Steve Jobs) | Attrs: {'manner': 'later'}\n",
      "  Extracted Rel: (Bob Iger) --[LEAD]--> (Disney) | Attrs: {}\n",
      "\n",
      "7. Extracting final entities...\n",
      "\n",
      "Final Entities (NER + Refined Coref):\n",
      "  - Original: 'Apple Inc.' (ORG), StartChar: 0 -> Resolved: 'Apple'\n",
      "  - Original: 'Steve Jobs' (PERSON), StartChar: 23 -> Resolved: 'Steve Jobs'\n",
      "  - Original: 'Steve Wozniak' (PERSON), StartChar: 38 -> Resolved: 'Steve Wozniak'\n",
      "  - Original: 'Cupertino' (GPE), StartChar: 65 -> Resolved: 'Cupertino'\n",
      "  - Original: 'Tim Cook' (PERSON), StartChar: 80 -> Resolved: 'Tim Cook'\n",
      "  - Original: 'Apple' (ORG), StartChar: 107 -> Resolved: 'Apple'\n",
      "  - Original: '2011' (DATE), StartChar: 116 -> Resolved: '2011'\n",
      "  - Original: 'IBM' (ORG), StartChar: 146 -> Resolved: 'IBM'\n",
      "  - Original: 'Apple' (ORG), StartChar: 155 -> Resolved: 'Apple'\n",
      "  - Original: 'iPhone' (ORG), StartChar: 182 -> Resolved: 'iPhone'\n",
      "  - Original: 'Google' (ORG), StartChar: 190 -> Resolved: 'Google'\n",
      "  - Original: 'Android' (ORG), StartChar: 220 -> Resolved: 'Android'\n",
      "  - Original: 'Steve Jobs' (PERSON), StartChar: 233 -> Resolved: 'Steve Jobs'\n",
      "  - Original: 'Pixar' (ORG), StartChar: 260 -> Resolved: 'Pixar'\n",
      "  - Original: 'Disney' (ORG), StartChar: 295 -> Resolved: 'Disney'\n",
      "  - Original: 'Bob Iger' (PERSON), StartChar: 303 -> Resolved: 'Bob Iger'\n",
      "  - Original: 'Disney' (ORG), StartChar: 318 -> Resolved: 'Disney'\n",
      "\n",
      "--- Processing Complete ---\n",
      "\n",
      "============================== FINAL RESULTS ==============================\n",
      "\n",
      "Entities (Resolved Name: Type):\n",
      "  - 2011: DATE\n",
      "  - Android: ORG\n",
      "  - Apple: ORG\n",
      "  - Bob Iger: PERSON\n",
      "  - Cupertino: GPE\n",
      "  - Disney: ORG\n",
      "  - Google: ORG\n",
      "  - IBM: ORG\n",
      "  - Pixar: ORG\n",
      "  - Steve Jobs: PERSON\n",
      "  - Steve Wozniak: PERSON\n",
      "  - Tim Cook: PERSON\n",
      "  - iPhone: ORG\n",
      "\n",
      "Relationships (Subject, Verb, Object, Attributes):\n",
      "  - (Apple) --[ESTABLISH_IN]--> (Cupertino) \n",
      "  - (Apple) --[PRODUCE]--> (iPhone) \n",
      "  - (Bob Iger) --[LEAD]--> (Disney) \n",
      "  - (Disney) --[GET]--> (Steve Jobs) | {'manner': 'later'}\n",
      "  - (Google) --[MAKE]--> (Android) \n",
      "  - (Tim Cook) --[WORK_AT]--> (IBM) | {'manner': 'previously'}\n",
      "EE:\n",
      "{'Apple': 'ORG', 'Steve Jobs': 'PERSON', 'Steve Wozniak': 'PERSON', 'Cupertino': 'GPE', 'Tim Cook': 'PERSON', '2011': 'DATE', 'IBM': 'ORG', 'iPhone': 'ORG', 'Google': 'ORG', 'Android': 'ORG', 'Pixar': 'ORG', 'Disney': 'ORG', 'Bob Iger': 'PERSON'}\n",
      "\n",
      "--- create_nodes_and_relationships: Creating Entity Nodes ---\n",
      "Entities: {'Apple': 'ORG', 'Steve Jobs': 'PERSON', 'Steve Wozniak': 'PERSON', 'Cupertino': 'GPE', 'Tim Cook': 'PERSON', '2011': 'DATE', 'IBM': 'ORG', 'iPhone': 'ORG', 'Google': 'ORG', 'Android': 'ORG', 'Pixar': 'ORG', 'Disney': 'ORG', 'Bob Iger': 'PERSON'}\n",
      "\n",
      "Processing entity: name='Apple', type='ORG'\n",
      "Executing query: 'MERGE (n:ORG {name: $name}) RETURN n' with parameters: {'name': 'Apple'}\n",
      "Stored node: ID=2, Labels=['ORG'], Properties={'name': 'Apple'}\n",
      "\n",
      "Processing entity: name='Steve Jobs', type='PERSON'\n",
      "Executing query: 'MERGE (n:PERSON {name: $name}) RETURN n' with parameters: {'name': 'Steve Jobs'}\n",
      "Stored node: ID=3, Labels=['PERSON'], Properties={'name': 'Steve Jobs'}\n",
      "\n",
      "Processing entity: name='Steve Wozniak', type='PERSON'\n",
      "Executing query: 'MERGE (n:PERSON {name: $name}) RETURN n' with parameters: {'name': 'Steve Wozniak'}\n",
      "Stored node: ID=4, Labels=['PERSON'], Properties={'name': 'Steve Wozniak'}\n",
      "\n",
      "Processing entity: name='Cupertino', type='GPE'\n",
      "Executing query: 'MERGE (n:GPE {name: $name}) RETURN n' with parameters: {'name': 'Cupertino'}\n",
      "Stored node: ID=5, Labels=['GPE'], Properties={'name': 'Cupertino'}\n",
      "\n",
      "Processing entity: name='Tim Cook', type='PERSON'\n",
      "Executing query: 'MERGE (n:PERSON {name: $name}) RETURN n' with parameters: {'name': 'Tim Cook'}\n",
      "Stored node: ID=6, Labels=['PERSON'], Properties={'name': 'Tim Cook'}\n",
      "\n",
      "Processing entity: name='2011', type='DATE'\n",
      "Executing query: 'MERGE (n:DATE {name: $name}) RETURN n' with parameters: {'name': '2011'}\n",
      "Stored node: ID=7, Labels=['DATE'], Properties={'name': '2011'}\n",
      "\n",
      "Processing entity: name='IBM', type='ORG'\n",
      "Executing query: 'MERGE (n:ORG {name: $name}) RETURN n' with parameters: {'name': 'IBM'}\n",
      "Stored node: ID=8, Labels=['ORG'], Properties={'name': 'IBM'}\n",
      "\n",
      "Processing entity: name='iPhone', type='ORG'\n",
      "Executing query: 'MERGE (n:ORG {name: $name}) RETURN n' with parameters: {'name': 'iPhone'}\n",
      "Stored node: ID=9, Labels=['ORG'], Properties={'name': 'iPhone'}\n",
      "\n",
      "Processing entity: name='Google', type='ORG'\n",
      "Executing query: 'MERGE (n:ORG {name: $name}) RETURN n' with parameters: {'name': 'Google'}\n",
      "Stored node: ID=10, Labels=['ORG'], Properties={'name': 'Google'}\n",
      "\n",
      "Processing entity: name='Android', type='ORG'\n",
      "Executing query: 'MERGE (n:ORG {name: $name}) RETURN n' with parameters: {'name': 'Android'}\n",
      "Stored node: ID=11, Labels=['ORG'], Properties={'name': 'Android'}\n",
      "\n",
      "Processing entity: name='Pixar', type='ORG'\n",
      "Executing query: 'MERGE (n:ORG {name: $name}) RETURN n' with parameters: {'name': 'Pixar'}\n",
      "Stored node: ID=12, Labels=['ORG'], Properties={'name': 'Pixar'}\n",
      "\n",
      "Processing entity: name='Disney', type='ORG'\n",
      "Executing query: 'MERGE (n:ORG {name: $name}) RETURN n' with parameters: {'name': 'Disney'}\n",
      "Stored node: ID=13, Labels=['ORG'], Properties={'name': 'Disney'}\n",
      "\n",
      "Processing entity: name='Bob Iger', type='PERSON'\n",
      "Executing query: 'MERGE (n:PERSON {name: $name}) RETURN n' with parameters: {'name': 'Bob Iger'}\n",
      "Stored node: ID=14, Labels=['PERSON'], Properties={'name': 'Bob Iger'}\n",
      "\n",
      "--- create_nodes_and_relationships: Creating Relationships ---\n",
      "Relationships: [('Apple', 'ESTABLISH_IN', 'Cupertino', {}), ('Apple', 'PRODUCE', 'iPhone', {}), ('Bob Iger', 'LEAD', 'Disney', {}), ('Disney', 'GET', 'Steve Jobs', {'manner': 'later'}), ('Google', 'MAKE', 'Android', {}), ('Tim Cook', 'WORK_AT', 'IBM', {'manner': 'previously'})]\n",
      "\n",
      "Processing relationship: subj='Apple', verb='ESTABLISH_IN', obj='Cupertino', attrs='{}'\n",
      "Executing query: MATCH (a) WHERE id(a)=2 MATCH (b) WHERE id(b)=5 ...\n",
      "Relationship merged/created successfully\n",
      "\n",
      "Processing relationship: subj='Apple', verb='PRODUCE', obj='iPhone', attrs='{}'\n",
      "Executing query: MATCH (a) WHERE id(a)=2 MATCH (b) WHERE id(b)=9 ...\n",
      "Relationship merged/created successfully\n",
      "\n",
      "Processing relationship: subj='Bob Iger', verb='LEAD', obj='Disney', attrs='{}'\n",
      "Executing query: MATCH (a) WHERE id(a)=14 MATCH (b) WHERE id(b)=13 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 2, column: 33, offset: 33} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:LEAD]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 3, column: 33, offset: 85} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:LEAD]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 2, column: 33, offset: 33} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:GET]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 3, column: 33, offset: 85} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:GET]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 2, column: 33, offset: 33} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:MAKE]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 3, column: 33, offset: 85} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:MAKE]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 2, column: 33, offset: 33} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:WORK_AT]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n",
      "04/21/2025 16:46:01 - WARNING - \t Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 3, column: 33, offset: 85} for query: '\\n                MATCH (a) WHERE id(a) = $subject_id\\n                MATCH (b) WHERE id(b) = $object_id\\n                MERGE (a)-[r:WORK_AT]->(b)\\n                ON CREATE SET r = $attributes\\n                ON MATCH SET r += $attributes \\n                '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship merged/created successfully\n",
      "\n",
      "Processing relationship: subj='Disney', verb='GET', obj='Steve Jobs', attrs='{'manner': 'later'}'\n",
      "Executing query: MATCH (a) WHERE id(a)=13 MATCH (b) WHERE id(b)=3 ...\n",
      "Relationship merged/created successfully\n",
      "\n",
      "Processing relationship: subj='Google', verb='MAKE', obj='Android', attrs='{}'\n",
      "Executing query: MATCH (a) WHERE id(a)=10 MATCH (b) WHERE id(b)=11 ...\n",
      "Relationship merged/created successfully\n",
      "\n",
      "Processing relationship: subj='Tim Cook', verb='WORK_AT', obj='IBM', attrs='{'manner': 'previously'}'\n",
      "Executing query: MATCH (a) WHERE id(a)=6 MATCH (b) WHERE id(b)=8 ...\n",
      "Relationship merged/created successfully\n",
      "\n",
      "Neo4j operation finished.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def create_graph(uri, username, password, entities, relationships):\n",
    "    \"\"\"\n",
    "    Connects to Neo4j and inserts entities and relationships.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "    \n",
    "    def create_nodes_and_relationships(tx):\n",
    "        # Create entity nodes\n",
    "        entity_nodes = {} # Stores mapping from entity name (string) to Neo4j Node object\n",
    "        print(\"\\n--- create_nodes_and_relationships: Creating Entity Nodes ---\")\n",
    "        print(f\"Entities: {entities}\")\n",
    "        for entity_name, entity_type in entities.items():\n",
    "            print(f\"\\nProcessing entity: name='{entity_name}', type='{entity_type}'\")\n",
    "            # MERGE finds or creates the node. RETURN n gives us the node back.\n",
    "            query = \"MERGE (n:{type} {{name: $name}}) RETURN n\".format(type=entity_type)\n",
    "            print(f\"Executing query: '{query}' with parameters: {{'name': '{entity_name}'}}\")\n",
    "            try:\n",
    "                result = tx.run(query, name=entity_name)\n",
    "                \n",
    "                # Use single() directly to get the record\n",
    "                record = result.single() \n",
    "                \n",
    "                if record:\n",
    "                    # Access the node using the alias 'n' from the RETURN clause\n",
    "                    node_object = record['n'] \n",
    "                    # Store the actual Neo4j Node object in the dictionary\n",
    "                    entity_nodes[entity_name] = node_object \n",
    "                    # You can print node details if needed for confirmation\n",
    "                    print(f\"Stored node: ID={node_object.id}, Labels={list(node_object.labels)}, Properties={dict(node_object)}\")\n",
    "                else:\n",
    "                    # This could happen if the transaction somehow failed before commit,\n",
    "                    # or if MERGE failed constraints, but it's less likely for this simple query.\n",
    "                    print(f\"WARN: No record returned from MERGE query for entity '{entity_name}'.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR creating/merging node '{entity_name}': {e}\")\n",
    "                raise  # Re-raise the exception to potentially abort the transaction\n",
    "\n",
    "        print(\"\\n--- create_nodes_and_relationships: Creating Relationships ---\")\n",
    "        print(f\"Relationships: {relationships}\")\n",
    "        # The relationship creation part should now work because entity_nodes will be populated correctly\n",
    "        for subj, verb, obj, attrs in relationships:\n",
    "            print(f\"\\nProcessing relationship: subj='{subj}', verb='{verb}', obj='{obj}', attrs='{attrs}'\")\n",
    "            # Ensure both subject and object entities were successfully retrieved/created\n",
    "            if subj in entity_nodes and obj in entity_nodes:\n",
    "                # Get the actual Node objects from the dictionary\n",
    "                subj_node = entity_nodes[subj]\n",
    "                obj_node = entity_nodes[obj]\n",
    "                \n",
    "                # Use internal IDs to match the nodes efficiently\n",
    "                query = \"\"\"\n",
    "                MATCH (a) WHERE id(a) = $subject_id\n",
    "                MATCH (b) WHERE id(b) = $object_id\n",
    "                MERGE (a)-[r:{relation}]->(b)\n",
    "                ON CREATE SET r = $attributes\n",
    "                ON MATCH SET r += $attributes \n",
    "                \"\"\".format(relation=verb) # Use MERGE for relationships too if you want to avoid duplicates\n",
    "                \n",
    "                print(f\"Executing query: MATCH (a) WHERE id(a)={subj_node.id} MATCH (b) WHERE id(b)={obj_node.id} ...\")\n",
    "                try:\n",
    "                    tx.run(\n",
    "                        query,\n",
    "                        subject_id=subj_node.id, # Pass the internal ID\n",
    "                        object_id=obj_node.id,   # Pass the internal ID\n",
    "                        attributes=attrs if attrs else {} # Ensure attributes is a dict\n",
    "                    )\n",
    "                    print(\"Relationship merged/created successfully\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR creating relationship ({subj})-[{verb}]->({obj}): {e}\")\n",
    "                    raise # Re-raise to potentially abort transaction\n",
    "            else:\n",
    "                # Print which entity was missing\n",
    "                missing = []\n",
    "                if subj not in entity_nodes: missing.append(subj)\n",
    "                if obj not in entity_nodes: missing.append(obj)\n",
    "                print(f\"WARN: Skipping relationship ({subj})--[{verb}]-->({obj}) because entity(ies) not found in entity_nodes dict: {', '.join(missing)}\")\n",
    "\n",
    "    # The rest of your function (session handling, driver close) remains the same\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(create_nodes_and_relationships)\n",
    "\n",
    "    driver.close()\n",
    "    print(\"\\nNeo4j operation finished.\") # Changed message slightly\n",
    "        \n",
    "# The rest of your existing code (import statements, helper functions,\n",
    "# process_text_to_graph_info_v2 function, and the main execution block\n",
    "# up to the point where extracted_relationships and extracted_entities are defined)\n",
    "# goes here.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 1. Load spaCy Model ---\n",
    "    print(f\"Loading spaCy model: {spacy_model}\")\n",
    "    try:\n",
    "        nlp = spacy.load(spacy_model)\n",
    "    except OSError:\n",
    "        print(f\"Error loading spaCy model '{spacy_model}'. Download it.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Initialize FastCoref Model ---\n",
    "    print(\"\\nInitializing FastCoref model...\")\n",
    "    # Determine device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(\"Attempting to use MPS (Apple Silicon GPU).\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "        print(\"Attempting to use CUDA GPU.\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"GPU not available, using CPU.\")\n",
    "\n",
    "    try:\n",
    "        # Try initializing with the determined device\n",
    "        fc_model = FCoref(fastcoref_model_name, nlp, device=device)\n",
    "        print(f\"Successfully initialized FastCoref on {device.upper()}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"{device.upper()} initialization failed ({e}). Falling back to CPU.\")\n",
    "        # Fallback to CPU if GPU fails\n",
    "        device = 'mps'\n",
    "        fc_model = FCoref(fastcoref_model_name, nlp=nlp, device=device)\n",
    "        print(\"Successfully initialized FastCoref on CPU.\")\n",
    "\n",
    "\n",
    "    # --- 3. Define Input Text ---\n",
    "    text = \"\"\"\n",
    "    Apple Inc., founded by Steve Jobs and Steve Wozniak, is based in Cupertino.\n",
    "    Tim Cook became the CEO of Apple in 2011. He previously worked at IBM.\n",
    "    Apple produces the popular iPhone. Google, its competitor, makes Android.\n",
    "    Steve Jobs also co-founded Pixar, which was later acquired by Disney. Bob Iger leads Disney.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 4. Run the Full Pipeline ---\n",
    "    extracted_relationships, extracted_entities = process_text_to_graph_info_v2(\n",
    "        text, nlp, fc_model\n",
    "    )\n",
    "\n",
    "    # --- 5. Display Results ---\n",
    "    print(\"\\n\" + \"=\"*30 + \" FINAL RESULTS \" + \"=\"*30)\n",
    "    print(\"\\nEntities (Resolved Name: Type):\")\n",
    "    if extracted_entities:\n",
    "        # Sort for consistent output\n",
    "        for name in sorted(extracted_entities.keys()):\n",
    "            print(f\"  - {name}: {extracted_entities[name]}\")\n",
    "    else:\n",
    "        print(\"  (No entities found)\")\n",
    "\n",
    "    print(\"\\nRelationships (Subject, Verb, Object, Attributes):\")\n",
    "    if extracted_relationships:\n",
    "        # Sort for consistent output\n",
    "        extracted_relationships.sort()\n",
    "        for rel in extracted_relationships:\n",
    "            subj, verb, obj, attrs = rel\n",
    "            attr_str = f\"| {attrs}\" if attrs else \"\"\n",
    "            print(f\"  - ({subj}) --[{verb}]--> ({obj}) {attr_str}\")\n",
    "    else:\n",
    "        print(\"  (No relationships found)\")\n",
    "\n",
    "    # --- 6. Insert into Neo4j ---\n",
    "    print(\"EE:\")\n",
    "    print(extracted_entities)\n",
    "    create_graph(uri, username, password, extracted_entities, extracted_relationships)\n",
    "\n",
    "    print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d68bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Querying relationships for: 'Apple'\n",
      "========================================\n",
      "\n",
      "Executing query: MATCH (e {name: $entity_name})-[r]-(other)\n",
      "        RETURN startNode(r) as start_node, r as relationship, endNode(r) as end_node with params: {'entity_name': 'Apple'}\n",
      "Query finished. Found 2 relationships for 'Apple'.\n",
      "[\n",
      "  {\n",
      "    \"start_node\": {\n",
      "      \"name\": \"Apple\"\n",
      "    },\n",
      "    \"relationship_type\": \"ESTABLISH_IN\",\n",
      "    \"relationship_props\": {},\n",
      "    \"end_node\": {\n",
      "      \"name\": \"Cupertino\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"start_node\": {\n",
      "      \"name\": \"Apple\"\n",
      "    },\n",
      "    \"relationship_type\": \"PRODUCE\",\n",
      "    \"relationship_props\": {},\n",
      "    \"end_node\": {\n",
      "      \"name\": \"iPhone\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "--- Graph-like format ---\n",
      "  (Apple) -[ESTABLISH_IN]-> (Cupertino)\n",
      "  (Apple) -[PRODUCE]-> (iPhone)\n",
      "\n",
      "========================================\n",
      "Querying relationships for: 'Steve Jobs'\n",
      "========================================\n",
      "\n",
      "Executing query: MATCH (e {name: $entity_name})-[r]-(other)\n",
      "        RETURN startNode(r) as start_node, r as relationship, endNode(r) as end_node with params: {'entity_name': 'Steve Jobs'}\n",
      "Query finished. Found 1 relationships for 'Steve Jobs'.\n",
      "[\n",
      "  {\n",
      "    \"start_node\": {\n",
      "      \"name\": \"Disney\"\n",
      "    },\n",
      "    \"relationship_type\": \"GET\",\n",
      "    \"relationship_props\": {\n",
      "      \"manner\": \"later\"\n",
      "    },\n",
      "    \"end_node\": {\n",
      "      \"name\": \"Steve Jobs\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "--- Graph-like format ---\n",
      "  (Disney) <-[GET {'manner': 'later'}]- (Steve Jobs)\n",
      "\n",
      "========================================\n",
      "Querying relationships for: 'Disney'\n",
      "========================================\n",
      "\n",
      "Executing query: MATCH (e {name: $entity_name})-[r]-(other)\n",
      "        RETURN startNode(r) as start_node, r as relationship, endNode(r) as end_node with params: {'entity_name': 'Disney'}\n",
      "Query finished. Found 2 relationships for 'Disney'.\n",
      "[\n",
      "  {\n",
      "    \"start_node\": {\n",
      "      \"name\": \"Bob Iger\"\n",
      "    },\n",
      "    \"relationship_type\": \"LEAD\",\n",
      "    \"relationship_props\": {},\n",
      "    \"end_node\": {\n",
      "      \"name\": \"Disney\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"start_node\": {\n",
      "      \"name\": \"Disney\"\n",
      "    },\n",
      "    \"relationship_type\": \"GET\",\n",
      "    \"relationship_props\": {\n",
      "      \"manner\": \"later\"\n",
      "    },\n",
      "    \"end_node\": {\n",
      "      \"name\": \"Steve Jobs\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "--- Graph-like format ---\n",
      "  (Bob Iger) <-[LEAD]- (Disney)\n",
      "  (Disney) -[GET {'manner': 'later'}]-> (Steve Jobs)\n",
      "\n",
      "========================================\n",
      "Querying relationships for: 'NonExistentEntity'\n",
      "========================================\n",
      "\n",
      "Executing query: MATCH (e {name: $entity_name})-[r]-(other)\n",
      "        RETURN startNode(r) as start_node, r as relationship, endNode(r) as end_node with params: {'entity_name': 'NonExistentEntity'}\n",
      "Query finished. Found 0 relationships for 'NonExistentEntity'.\n",
      "No relationships found for entity 'NonExistentEntity' (or entity does not exist).\n"
     ]
    }
   ],
   "source": [
    "def get_entity_relationships(uri, username, password, entity_name):\n",
    "    \"\"\"\n",
    "    Queries Neo4j to find all relationships connected to a specific entity,\n",
    "    identified by its 'name' property.\n",
    "\n",
    "    Args:\n",
    "        uri (str): The connection URI for the Neo4j database.\n",
    "        username (str): The username for database authentication.\n",
    "        password (str): The password for database authentication.\n",
    "        entity_name (str): The value of the 'name' property of the entity\n",
    "                           to query relationships for.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a\n",
    "              relationship and contains:\n",
    "              - 'start_node': Properties (dict) of the start node.\n",
    "              - 'relationship_type': The type (str) of the relationship.\n",
    "              - 'relationship_props': Properties (dict) of the relationship itself.\n",
    "              - 'end_node': Properties (dict) of the end node.\n",
    "              Returns an empty list if the entity is not found or has no\n",
    "              relationships.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "    relationships_list = []\n",
    "\n",
    "    # Internal function to run within a transaction\n",
    "    def _find_relationships_tx(tx, name):\n",
    "        # This Cypher query finds the entity 'e' by its name.\n",
    "        # Then it matches any relationship '[r]' connected to 'e',\n",
    "        # regardless of direction (which is why there are no arrows: -[r]-).\n",
    "        # 'other' represents the node at the other end of the relationship.\n",
    "        # We RETURN the actual start node, the relationship itself, and the actual end node.\n",
    "        query = \"\"\"\n",
    "        MATCH (e {name: $entity_name})-[r]-(other)\n",
    "        RETURN startNode(r) as start_node, r as relationship, endNode(r) as end_node\n",
    "        \"\"\"\n",
    "        print(f\"\\nExecuting query: {query.strip()} with params: {{'entity_name': '{name}'}}\")\n",
    "        result = tx.run(query, entity_name=name)\n",
    "\n",
    "        # Process each record (relationship) found\n",
    "        found_relationships = []\n",
    "        for record in result:\n",
    "            start_node = record['start_node']\n",
    "            relationship = record['relationship']\n",
    "            end_node = record['end_node']\n",
    "\n",
    "            # Store the details in a dictionary\n",
    "            found_relationships.append({\n",
    "                # Convert node/relationship properties to standard dictionaries\n",
    "                'start_node': dict(start_node.items()),\n",
    "                'relationship_type': relationship.type,\n",
    "                'relationship_props': dict(relationship.items()),\n",
    "                'end_node': dict(end_node.items())\n",
    "            })\n",
    "        return found_relationships\n",
    "\n",
    "    try:\n",
    "        # Use a read transaction as we are only querying data\n",
    "        with driver.session() as session:\n",
    "            relationships_list = session.execute_read(_find_relationships_tx, entity_name)\n",
    "        print(f\"Query finished. Found {len(relationships_list)} relationships for '{entity_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred querying relationships for '{entity_name}': {e}\")\n",
    "    finally:\n",
    "        # Ensure the driver connection is closed\n",
    "        driver.close()\n",
    "\n",
    "    return relationships_list\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    entities_to_query = [\"Apple\", \"Steve Jobs\", \"Disney\", \"NonExistentEntity\"]\n",
    "\n",
    "    for entity in entities_to_query:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"Querying relationships for: '{entity}'\")\n",
    "        print(\"=\"*40)\n",
    "        found_rels = get_entity_relationships(uri, username, password, entity)\n",
    "\n",
    "        if found_rels:\n",
    "            # Pretty print the list of relationship dictionaries\n",
    "            print(json.dumps(found_rels, indent=2))\n",
    "\n",
    "            # Or print in a more readable graph-like format:\n",
    "            print(\"\\n--- Graph-like format ---\")\n",
    "            for rel in found_rels:\n",
    "                start = rel['start_node'].get('name', 'Unknown') # Use .get for safety\n",
    "                end = rel['end_node'].get('name', 'Unknown')\n",
    "                rel_type = rel['relationship_type']\n",
    "                rel_props = rel['relationship_props']\n",
    "                prop_str = f\" {rel_props}\" if rel_props else \"\" # Show props if they exist\n",
    "\n",
    "                # Check if the queried entity is the start or end node to show directionality clearly\n",
    "                if start == entity:\n",
    "                    print(f\"  ({start}) -[{rel_type}{prop_str}]-> ({end})\")\n",
    "                elif end == entity:\n",
    "                     print(f\"  ({start}) <-[{rel_type}{prop_str}]- ({end})\")\n",
    "                else:\n",
    "                     # Should not happen with the query used, but good for robustness\n",
    "                     print(f\"  ({start}) -[{rel_type}{prop_str}]- ({end})  (entity '{entity}' involved but direction unclear?)\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No relationships found for entity '{entity}' (or entity does not exist).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a424a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/21/2025 16:46:01 - INFO - \t WordNet data found.\n",
      "04/21/2025 16:46:01 - INFO - \t Loading spaCy model: en_core_web_lg\n",
      "04/21/2025 16:46:02 - INFO - \t spaCy model loaded successfully.\n",
      "04/21/2025 16:46:02 - INFO - \t Initializing FastCoref model...\n",
      "04/21/2025 16:46:02 - INFO - \t Attempting to use MPS (Apple Silicon GPU).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/21/2025 16:46:04 - INFO - \t missing_keys: []\n",
      "04/21/2025 16:46:04 - INFO - \t unexpected_keys: []\n",
      "04/21/2025 16:46:04 - INFO - \t mismatched_keys: []\n",
      "04/21/2025 16:46:04 - INFO - \t error_msgs: []\n",
      "04/21/2025 16:46:04 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/21/2025 16:46:04 - INFO - \t Successfully initialized FastCoref model 'biu-nlp/f-coref' on MPS.\n",
      "04/21/2025 16:46:04 - INFO - \t --- Starting Text Processing Pipeline ---\n",
      "04/21/2025 16:46:04 - INFO - \t Processing text: 'Apple Inc., founded by Steve Jobs and Steve Wozniak in 1976, is based in Cupertino, California.\n",
      "    ...'\n",
      "04/21/2025 16:46:04 - INFO - \t Running spaCy NLP pipeline...\n",
      "04/21/2025 16:46:04 - INFO - \t spaCy processing complete.\n",
      "04/21/2025 16:46:04 - INFO - \t Extracting initial NER entities...\n",
      "04/21/2025 16:46:04 - INFO - \t Found 19 initial unique entity texts.\n",
      "04/21/2025 16:46:04 - INFO - \t Running FastCoref pipeline...\n",
      "04/21/2025 16:46:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 115.50 examples/s]\n",
      "04/21/2025 16:46:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "04/21/2025 16:46:05 - INFO - \t FastCoref processing complete. Found 4 clusters.\n",
      "04/21/2025 16:46:05 - INFO - \t Building initial coreference map...\n",
      "04/21/2025 16:46:05 - INFO - \t Refining coreference map using NER entities...\n",
      "04/21/2025 16:46:05 - INFO - \t Coreference map refined.\n",
      "04/21/2025 16:46:05 - INFO - \t Extracting enhanced relationships...\n",
      "04/21/2025 16:46:05 - INFO - \t Starting relationship extraction...\n",
      "04/21/2025 16:46:05 - INFO - \t   Extracted Rel: ('1976', 'ESTABLISH_IN', 'Cupertino', {})\n",
      "04/21/2025 16:46:05 - INFO - \t   Extracted Rel: ('Tim Cook', 'BECOME', 'the CEO', {'time': 'August 2011'})\n",
      "04/21/2025 16:46:05 - INFO - \t   Extracted Rel: ('Tim Cook', 'WORK_AT', 'IBM Corp.\\n    Apple', {'manner': 'previously'})\n",
      "04/21/2025 16:46:05 - INFO - \t   Extracted Rel: ('1976', 'MAKE', 'the Android operating system', {})\n",
      "04/21/2025 16:46:05 - INFO - \t   Extracted Rel: ('Steve Jobs', 'CO', 'Pixar Animation Studios', {'manner': 'also'})\n",
      "04/21/2025 16:46:05 - INFO - \t   Extracted Rel: ('The Walt Disney Company', 'GET', 'which', {'manner': 'later', 'time': '2006'})\n",
      "04/21/2025 16:46:05 - INFO - \t   Extracted Rel: ('Bob Iger', 'LEAD', 'The Walt Disney Company', {'manner': 'currently'})\n",
      "04/21/2025 16:46:05 - INFO - \t Finished relationship extraction. Found 7 relationships.\n",
      "04/21/2025 16:46:05 - INFO - \t Extracting final entities with resolved names...\n",
      "04/21/2025 16:46:05 - INFO - \t Extracting final entities using refined coref map...\n",
      "04/21/2025 16:46:05 - WARNING - \t Conflicting labels for '1976'. Keeping existing 'ORG', ignoring new label 'DATE' from original text '1976'.\n",
      "04/21/2025 16:46:05 - WARNING - \t Conflicting labels for 'Steve Jobs'. Keeping existing 'PERSON', ignoring new label 'ORG' from original text 'Jobs'.\n",
      "04/21/2025 16:46:05 - INFO - \t Finished entity extraction. Found 15 unique final entities.\n",
      "04/21/2025 16:46:05 - INFO - \t --- Text Processing Pipeline Complete ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== FINAL RESULTS ==============================\n",
      "\n",
      "Entities (Resolved Name: Type):\n",
      "  - 1976: ORG\n",
      "  - 2006: DATE\n",
      "  - Android: ORG\n",
      "  - August 2011: DATE\n",
      "  - Bob Iger: PERSON\n",
      "  - California: GPE\n",
      "  - Cupertino: GPE\n",
      "  - Google LLC: ORG\n",
      "  - IBM Corp.\n",
      "    Apple: ORG\n",
      "  - Pixar Animation Studios: ORG\n",
      "  - Steve Jobs: PERSON\n",
      "  - Steve Wozniak: PERSON\n",
      "  - The Walt Disney Company: ORG\n",
      "  - Tim Cook: PERSON\n",
      "  - iPhone: ORG\n",
      "\n",
      "Relationships (Subject, Verb, Object, Attributes):\n",
      "  - (1976) --[ESTABLISH_IN]--> (Cupertino) \n",
      "  - (1976) --[MAKE]--> (the Android operating system) \n",
      "  - (Bob Iger) --[LEAD]--> (The Walt Disney Company) | Attributes: {'manner': 'currently'}\n",
      "  - (Steve Jobs) --[CO]--> (Pixar Animation Studios) | Attributes: {'manner': 'also'}\n",
      "  - (The Walt Disney Company) --[GET]--> (which) | Attributes: {'manner': 'later', 'time': '2006'}\n",
      "  - (Tim Cook) --[BECOME]--> (the CEO) | Attributes: {'time': 'August 2011'}\n",
      "  - (Tim Cook) --[WORK_AT]--> (IBM Corp.\n",
      "    Apple) | Attributes: {'manner': 'previously'}\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from fastcoref import FCoref\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import logging # Import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "spacy_model = \"en_core_web_lg\"\n",
    "# Try the potentially more accurate LingMess model\n",
    "fastcoref_model_name = \"biu-nlp/f-coref\"\n",
    "# fastcoref_model_name = 'lingmess-coref' # Alternative potentially more accurate but slower\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Replace print statements with logging for better control\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Download NLTK WordNet Data (if needed) ---\n",
    "try:\n",
    "    wordnet.synsets('computer')\n",
    "    logging.info(\"WordNet data found.\")\n",
    "except LookupError:\n",
    "    logging.info(\"Downloading NLTK WordNet data...\")\n",
    "    nltk.download('wordnet')\n",
    "    logging.info(\"WordNet download complete.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during WordNet check/download: {e}\")\n",
    "    # Decide if you want to exit or continue without WordNet\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_verb_synset_lemma(verb_token):\n",
    "    \"\"\"Gets the first lemma of the first WordNet synset for a verb token.\"\"\"\n",
    "    if not verb_token or verb_token.pos_ != \"VERB\":\n",
    "        return None\n",
    "    verb_lemma = verb_token.lemma_\n",
    "    try:\n",
    "        synsets = wordnet.synsets(verb_lemma, pos=wordnet.VERB)\n",
    "        if synsets:\n",
    "            # Use the first lemma of the most common sense\n",
    "            normalized = synsets[0].lemmas()[0].name()\n",
    "            # Use uppercase and replace underscores for Neo4j convention\n",
    "            return normalized.replace('_', '').upper()\n",
    "        else:\n",
    "            # Fallback to the original lemma if not found in WordNet\n",
    "            return verb_lemma.upper()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error normalizing verb '{verb_lemma}' with WordNet: {e}. Falling back to lemma.\")\n",
    "        return verb_lemma.upper() # Fallback on error\n",
    "\n",
    "def build_coref_map(text, clusters_indices):\n",
    "    \"\"\"\n",
    "    Builds an initial map from the start character index of a mention\n",
    "    to the raw text of the representative mention in its cluster.\n",
    "    \"\"\"\n",
    "    char_coref_mapping = {}\n",
    "    if not clusters_indices:\n",
    "        return char_coref_mapping\n",
    "\n",
    "    for cluster in clusters_indices:\n",
    "        if not cluster: continue\n",
    "        # The first mention in the cluster is usually the representative one\n",
    "        rep_start_char, rep_end_char = cluster[0]\n",
    "        # Store the raw text initially\n",
    "        rep_text = text[rep_start_char:rep_end_char].strip()\n",
    "        for mention_start_char, mention_end_char in cluster:\n",
    "            char_coref_mapping[mention_start_char] = rep_text\n",
    "    return char_coref_mapping\n",
    "\n",
    "def refine_coref_map_with_entities(initial_char_coref_map, initial_entities):\n",
    "    \"\"\"\n",
    "    Refines the coref map by replacing representative mentions\n",
    "    with known NER entity names where possible, prioritizing exact matches\n",
    "    and then shorter entity names contained within longer mentions.\n",
    "\n",
    "    Args:\n",
    "        initial_char_coref_map (dict): Map {start_char: representative_mention_text}\n",
    "        initial_entities (dict): Map {entity_text: label} from initial NER pass\n",
    "\n",
    "    Returns:\n",
    "        dict: Refined map {start_char: resolved_entity_name_or_original_rep_text}\n",
    "    \"\"\"\n",
    "    refined_map = initial_char_coref_map.copy()\n",
    "    representative_texts = set(initial_char_coref_map.values())\n",
    "    entity_texts_set = set(initial_entities.keys())\n",
    "    # Sort known entities by length (shortest first) to prefer shorter names\n",
    "    sorted_entity_texts = sorted(list(entity_texts_set), key=len)\n",
    "\n",
    "    rep_text_to_resolved_name = {}\n",
    "\n",
    "    for rep_text in representative_texts:\n",
    "        resolved_name = rep_text # Default to original representative text\n",
    "\n",
    "        # Priority 1: Check if the representative text itself is a known entity\n",
    "        if rep_text in entity_texts_set:\n",
    "            resolved_name = rep_text # Already a good name\n",
    "        else:\n",
    "            # Priority 2: Find the shortest known entity contained within the representative text\n",
    "            best_match = None\n",
    "            for short_entity_name in sorted_entity_texts:\n",
    "                 # Check for containment, ensure it's a meaningful substring\n",
    "                 # (e.g., check word boundaries or length threshold)\n",
    "                if short_entity_name in rep_text and len(short_entity_name) >= 3: # Basic check\n",
    "                    # More robust check: Use word boundaries?\n",
    "                    # import re\n",
    "                    # if re.search(r'\\b' + re.escape(short_entity_name) + r'\\b', rep_text):\n",
    "                    best_match = short_entity_name\n",
    "                    break # Found the shortest contained entity, use it\n",
    "            if best_match:\n",
    "                resolved_name = best_match\n",
    "            # else: resolved_name remains the original rep_text\n",
    "\n",
    "        rep_text_to_resolved_name[rep_text] = resolved_name\n",
    "\n",
    "    # Create the final refined map using the resolved names\n",
    "    final_refined_map = {}\n",
    "    for char_idx, original_rep_text in initial_char_coref_map.items():\n",
    "        final_refined_map[char_idx] = rep_text_to_resolved_name.get(original_rep_text, original_rep_text)\n",
    "\n",
    "    return final_refined_map\n",
    "\n",
    "\n",
    "def get_resolved_text_for_span(span, refined_char_coref_map):\n",
    "    \"\"\"\n",
    "    Resolves the text of a spaCy Span using the refined coref map,\n",
    "    checking the span's start character index. Falls back to span text.\n",
    "    \"\"\"\n",
    "    if not span: return None\n",
    "    # Use start_char as the key for the coref map\n",
    "    return refined_char_coref_map.get(span.start_char, span.text.strip())\n",
    "\n",
    "def get_resolved_text_for_token(token, refined_char_coref_map):\n",
    "     \"\"\"\n",
    "     Resolves the text for a single token using the coref map (less preferred than resolving spans).\n",
    "     Falls back to token text.\n",
    "     \"\"\"\n",
    "     if not token: return None\n",
    "     # Use token's character index (idx) as the key\n",
    "     return refined_char_coref_map.get(token.idx, token.text.strip())\n",
    "\n",
    "\n",
    "def get_entity_span_for_token(token, doc):\n",
    "    \"\"\"Finds the encompassing NER entity span for a token, if any.\"\"\"\n",
    "    # Check if the token itself is part of an entity\n",
    "    if token.ent_type_:\n",
    "        # Find the span this token belongs to\n",
    "        for ent in doc.ents:\n",
    "             # Check token index range inclusion\n",
    "            if token.i >= ent.start and token.i < ent.end:\n",
    "                return ent\n",
    "    return None # Token is not part of any detected entity\n",
    "\n",
    "def get_best_span_for_token(token, doc):\n",
    "    \"\"\"\n",
    "    Tries to find the best span representation for a token:\n",
    "    1. The full NER entity span if the token is part of one.\n",
    "    2. The noun chunk the token belongs to, if any.\n",
    "    3. The token itself as a fallback.\n",
    "    \"\"\"\n",
    "    ent_span = get_entity_span_for_token(token, doc)\n",
    "    if ent_span:\n",
    "        return ent_span\n",
    "\n",
    "    # Check if the token is part of a noun chunk\n",
    "    # Note: Noun chunks might span across multiple tokens\n",
    "    # We want the chunk that *contains* this specific token\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if token.i >= chunk.start and token.i < chunk.end:\n",
    "            return chunk # Return the full noun chunk span\n",
    "\n",
    "    # Fallback to the token itself (will just be its text)\n",
    "    # Returning the token makes the caller handle text extraction\n",
    "    return token\n",
    "\n",
    "\n",
    "def extract_relation_attributes(verb_token):\n",
    "    \"\"\"\n",
    "    Extracts attributes (time, location, manner) linked to a verb/action token\n",
    "    by checking its children in the dependency tree.\n",
    "    \"\"\"\n",
    "    attributes = {}\n",
    "    # Also check ancestors for attributes attached higher up (e.g., sentence-level adverbs)\n",
    "    # Be careful not to go too far up, maybe limit to the sentence?\n",
    "    tokens_to_check = list(verb_token.children) #+ list(verb_token.ancestors)\n",
    "\n",
    "    for child in tokens_to_check: # Check children first\n",
    "        dep = child.dep_\n",
    "        text_lower = child.text.lower()\n",
    "\n",
    "        # Time: Look for temporal modifiers (advmod) or prepositions governing DATE/TIME entities or numbers\n",
    "        if dep == 'advmod' and child.ent_type_ in ['DATE', 'TIME']:\n",
    "             attributes['time'] = get_best_span_for_token(child, child.doc).text.strip()\n",
    "        elif dep == 'prep' and text_lower in ['in', 'on', 'at', 'during', 'since', 'until', 'before', 'after']:\n",
    "            for grandchild in child.children:\n",
    "                if grandchild.dep_ == 'pobj':\n",
    "                    pobj_span = get_best_span_for_token(grandchild, grandchild.doc)\n",
    "                    # Check if the object is a recognized DATE/TIME entity or resembles a time expression\n",
    "                    if pobj_span.label_ in ['DATE', 'TIME'] if hasattr(pobj_span, 'label_') else False:\n",
    "                         attributes['time'] = pobj_span.text.strip()\n",
    "                         break\n",
    "                    elif grandchild.like_num and grandchild.text.isdigit() and len(grandchild.text) == 4 : # Simple year check\n",
    "                          attributes['time'] = grandchild.text.strip()\n",
    "                          break\n",
    "                    # Add more checks? e.g., phrases like \"next week\"\n",
    "\n",
    "        # Location: Look for prepositions governing GPE/LOC/FAC entities or locative advmod\n",
    "        elif dep == 'advmod' and child.ent_type_ in ['GPE', 'LOC', 'FAC']:\n",
    "             attributes['location'] = get_best_span_for_token(child, child.doc).text.strip()\n",
    "        elif dep == 'prep' and text_lower in ['in', 'at', 'on', 'near', 'from', 'to', 'within', 'near', 'based_in']: # Added 'based_in' etc. if needed\n",
    "             for grandchild in child.children:\n",
    "                 if grandchild.dep_ == 'pobj':\n",
    "                    pobj_span = get_best_span_for_token(grandchild, grandchild.doc)\n",
    "                    if pobj_span.label_ in ['GPE', 'LOC', 'FAC'] if hasattr(pobj_span, 'label_') else False:\n",
    "                        attributes['location'] = pobj_span.text.strip()\n",
    "                        break # Found location\n",
    "\n",
    "        # Manner: Look for adverbial modifiers (advmod) - typically adjectives modifying verbs\n",
    "        elif dep == 'advmod' and child.pos_ == 'ADV':\n",
    "            # Append manner adverbs, handle multiple ones\n",
    "            attributes.setdefault('manner', []).append(child.text.strip())\n",
    "\n",
    "    # Combine manner adverbs if multiple found\n",
    "    if 'manner' in attributes:\n",
    "        attributes['manner'] = \" \".join(attributes['manner'])\n",
    "\n",
    "    return attributes\n",
    "\n",
    "\n",
    "def extract_enhanced_relationships(doc, refined_char_coref_map):\n",
    "    \"\"\"\n",
    "    Extracts relationships using dependency parsing, coreference resolution,\n",
    "    and attribute extraction. Attempts to resolve entities to their best representation.\n",
    "    \"\"\"\n",
    "    relationships = []\n",
    "    processed_verbs = set() # Avoid processing compound verbs multiple times\n",
    "\n",
    "    logging.info(\"Starting relationship extraction...\")\n",
    "    for token in doc:\n",
    "        if token.i in processed_verbs: continue\n",
    "\n",
    "        # --- Trigger based on Verbs ---\n",
    "        if token.pos_ == \"VERB\":\n",
    "            subject_token = None\n",
    "            object_token = None\n",
    "            passive = False\n",
    "            prep_text = None # Preposition connecting verb and object (e.g., \"worked at\")\n",
    "            main_verb_token = token # Assume current token is main verb initially\n",
    "\n",
    "            # --- Find Subject ---\n",
    "            # Check children for nominal subjects (nsubj) or passive nominal subjects (nsubjpass)\n",
    "            potential_subjects = [child for child in token.children if \"subj\" in child.dep_]\n",
    "            if potential_subjects:\n",
    "                subject_token = potential_subjects[0] # Take the first subject found\n",
    "                if subject_token.dep_ == \"nsubjpass\":\n",
    "                    passive = True\n",
    "            else:\n",
    "                 # If no subject found attached to this verb, check if it's an auxiliary or part of a clause\n",
    "                 # and find the subject attached to the main verb it modifies.\n",
    "                 if token.dep_ in [\"aux\", \"auxpass\", \"xcomp\", \"ccomp\", \"advcl\"] and token.head.pos_ == \"VERB\":\n",
    "                     main_verb_token = token.head # The head is the main verb\n",
    "                     # Mark related verb tokens to avoid re-processing them individually\n",
    "                     processed_verbs.add(main_verb_token.i)\n",
    "                     for aux_child in token.head.children:\n",
    "                         if token.i >= aux_child.left_edge.i and token.i <= aux_child.right_edge.i:\n",
    "                            processed_verbs.add(aux_child.i) # Mark other verbs in the phrase too\n",
    "\n",
    "                     # Find subject attached to the actual main verb\n",
    "                     potential_head_subjects = [child for child in main_verb_token.children if \"subj\" in child.dep_]\n",
    "                     if potential_head_subjects:\n",
    "                         subject_token = potential_head_subjects[0]\n",
    "                         if subject_token.dep_ == \"nsubjpass\": passive = True\n",
    "\n",
    "\n",
    "            if not subject_token: continue # Cannot form relationship without a subject\n",
    "\n",
    "            # Ensure the main verb is correctly identified (not an auxiliary)\n",
    "            if main_verb_token.dep_ in [\"aux\", \"auxpass\"]: continue # Already processed via its head\n",
    "\n",
    "            # --- Find Object ---\n",
    "            # Look for direct object (dobj), prepositional object (pobj), attribute (attr),\n",
    "            # clausal complement (ccomp), open clausal complement (xcomp), or agent in passive voice.\n",
    "            potential_objects = []\n",
    "            for child in main_verb_token.children:\n",
    "                 # Direct object\n",
    "                 if child.dep_ == \"dobj\":\n",
    "                     potential_objects.append((child, None)) # (token, prep_text)\n",
    "                     break # Prioritize direct object\n",
    "                 # Attribute/predicate nominative (e.g., \"Tim Cook is CEO\")\n",
    "                 elif child.dep_ in [\"attr\", \"oprd\"]:\n",
    "                      potential_objects.append((child, None))\n",
    "                      break\n",
    "                 # Clausal complements (can sometimes be treated as objects)\n",
    "                 # elif child.dep_ in [\"ccomp\", \"xcomp\"]:\n",
    "                 #     # Find the verb or main noun within the complement? More complex.\n",
    "                 #     potential_objects.append((child, None)) # Simplification\n",
    "                 #     break\n",
    "                 # Prepositional Object\n",
    "                 elif child.dep_ == \"prep\":\n",
    "                     prep_token = child\n",
    "                     for grandchild in prep_token.children:\n",
    "                         if grandchild.dep_ == \"pobj\":\n",
    "                              potential_objects.append((grandchild, prep_token.text.lower()))\n",
    "                              # Don't break here, could have multiple prep phrases attached to verb\n",
    "                 # Agent in passive voice (\"by X\")\n",
    "                 elif passive and child.dep_ == \"agent\":\n",
    "                     for grandchild in child.children:\n",
    "                          if grandchild.dep_ == \"pobj\":\n",
    "                               # In passive: agent is logical subject, original subject is logical object\n",
    "                               object_token = subject_token # The original subject becomes the object\n",
    "                               subject_token = grandchild # The agent ('pobj' of 'by') becomes the subject\n",
    "                               passive = False # Treat as active now with swapped roles\n",
    "                               potential_objects = [] # Clear previous finds as roles are swapped\n",
    "                               break # Found the agent, stop searching object for main verb\n",
    "                     if object_token: break # Agent found and roles swapped\n",
    "\n",
    "            # Select the primary object if found\n",
    "            if not object_token and potential_objects: # If roles weren't swapped by passive agent\n",
    "                 # Prioritize objects: dobj/attr > pobj\n",
    "                 # This simple logic takes the first found object based on loop order above.\n",
    "                 # A more sophisticated approach might score or prioritize based on dependency type.\n",
    "                 object_token, prep_text = potential_objects[0]\n",
    "\n",
    "\n",
    "            if not object_token: continue # Require both subject and object\n",
    "\n",
    "\n",
    "            # --- Resolve Entities using best spans and coref map ---\n",
    "            subj_span = get_best_span_for_token(subject_token, doc)\n",
    "            obj_span = get_best_span_for_token(object_token, doc)\n",
    "\n",
    "            # Use the refined coref map preferentially, fall back to span text\n",
    "            resolved_subj = get_resolved_text_for_span(subj_span, refined_char_coref_map) if isinstance(subj_span, (spacy.tokens.Span, spacy.tokens.Token)) else subject_token.text.strip()\n",
    "            resolved_obj = get_resolved_text_for_span(obj_span, refined_char_coref_map) if isinstance(obj_span, (spacy.tokens.Span, spacy.tokens.Token)) else object_token.text.strip()\n",
    "\n",
    "\n",
    "            # If resolution failed or resulted in empty string, try token text directly\n",
    "            if not resolved_subj: resolved_subj = subject_token.text.strip()\n",
    "            if not resolved_obj: resolved_obj = object_token.text.strip()\n",
    "\n",
    "\n",
    "            # --- Determine Relation Phrase ---\n",
    "            # Normalize the main verb using WordNet\n",
    "            relation_phrase = get_verb_synset_lemma(main_verb_token) or main_verb_token.lemma_.upper()\n",
    "\n",
    "            # Enhance relation with preposition (e.g., WORK_AT) or particle (e.g., SET_UP)\n",
    "            if prep_text:\n",
    "                relation_phrase = f\"{relation_phrase}_{prep_text.upper()}\"\n",
    "            else:\n",
    "                 # Check for particle attached to the main verb\n",
    "                 for child in main_verb_token.children:\n",
    "                      if child.dep_ == 'prt': # Phrasal verb particle\n",
    "                           relation_phrase = f\"{relation_phrase}_{child.text.upper()}\"\n",
    "                           break\n",
    "\n",
    "\n",
    "            # --- Extract Attributes ---\n",
    "            # Get attributes associated with the *main* verb token\n",
    "            attributes = extract_relation_attributes(main_verb_token)\n",
    "\n",
    "            # --- Add Relationship ---\n",
    "            # Ensure subject and object are different and non-empty\n",
    "            if resolved_subj and resolved_obj and resolved_subj.lower() != resolved_obj.lower():\n",
    "                rel_tuple = (resolved_subj, relation_phrase, resolved_obj, attributes)\n",
    "                logging.info(f\"  Extracted Rel: {rel_tuple}\")\n",
    "                relationships.append(rel_tuple)\n",
    "\n",
    "    logging.info(f\"Finished relationship extraction. Found {len(relationships)} relationships.\")\n",
    "    return relationships\n",
    "\n",
    "\n",
    "def extract_final_entities(doc, refined_char_coref_map):\n",
    "    \"\"\"\n",
    "    Extracts Named Entities and resolves their names using the *refined* coref map.\n",
    "    Returns a dictionary: {resolved_entity_name: entity_label}\n",
    "    Handles potential label conflicts for the same resolved name.\n",
    "    \"\"\"\n",
    "    final_entities = {}\n",
    "    logging.info(\"Extracting final entities using refined coref map...\")\n",
    "    for ent in doc.ents:\n",
    "        # Resolve using the map, fallback to original text\n",
    "        resolved_name = refined_char_coref_map.get(ent.start_char, ent.text.strip())\n",
    "        label = ent.label_\n",
    "        # logging.debug(f\"  - Original: '{ent.text}' ({label}), StartChar: {ent.start_char} -> Resolved: '{resolved_name}'\")\n",
    "\n",
    "        # Check for conflicts: if name exists with a different label\n",
    "        if resolved_name in final_entities:\n",
    "            existing_label = final_entities[resolved_name]\n",
    "            if existing_label != label:\n",
    "                # Conflict resolution strategy:\n",
    "                # - Keep the first label encountered?\n",
    "                # - Keep the label from the longer/shorter original span?\n",
    "                # - Prioritize certain labels (e.g., ORG over PERSON if ambiguous)?\n",
    "                # Simple strategy: Keep the first one encountered and log a warning.\n",
    "                logging.warning(f\"Conflicting labels for '{resolved_name}'. Keeping existing '{existing_label}', ignoring new label '{label}' from original text '{ent.text}'.\")\n",
    "            # If labels are the same, no action needed.\n",
    "        else:\n",
    "            # Add the new entity and its label\n",
    "            final_entities[resolved_name] = label\n",
    "\n",
    "    logging.info(f\"Finished entity extraction. Found {len(final_entities)} unique final entities.\")\n",
    "    return final_entities\n",
    "\n",
    "\n",
    "# --- Main Pipeline Function ---\n",
    "\n",
    "def process_text_to_graph_info(text, spacy_nlp, fastcoref_model):\n",
    "    \"\"\"\n",
    "    Main pipeline to process text and extract entities and relationships.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting Text Processing Pipeline ---\")\n",
    "    if not text or not text.strip():\n",
    "        logging.warning(\"Input text is empty or whitespace only.\")\n",
    "        return [], {}\n",
    "\n",
    "    stripped_text = text.strip()\n",
    "    logging.info(f\"Processing text: '{stripped_text[:100]}...'\") # Log snippet\n",
    "\n",
    "    # 1. Process with spaCy (NER, POS, Dependency Parsing)\n",
    "    logging.info(\"Running spaCy NLP pipeline...\")\n",
    "    doc = spacy_nlp(stripped_text)\n",
    "    logging.info(\"spaCy processing complete.\")\n",
    "\n",
    "    # 2. Initial NER Extraction (used to help refine coref)\n",
    "    logging.info(\"Extracting initial NER entities...\")\n",
    "    initial_entities = {ent.text.strip(): ent.label_ for ent in doc.ents}\n",
    "    logging.info(f\"Found {len(initial_entities)} initial unique entity texts.\")\n",
    "    # logging.debug(f\"Initial entities: {initial_entities}\") # Debug level\n",
    "\n",
    "    # 3. Run Coreference Resolution\n",
    "    logging.info(\"Running FastCoref pipeline...\")\n",
    "    # Ensure text matches exactly what spaCy processed if models expect consistency\n",
    "    try:\n",
    "        preds = fastcoref_model.predict(texts=[stripped_text])\n",
    "        result = preds[0]\n",
    "        clusters_indices = result.get_clusters(as_strings=False)\n",
    "        # clusters_strings = result.get_clusters(as_strings=True) # For logging if needed\n",
    "        logging.info(f\"FastCoref processing complete. Found {len(clusters_indices)} clusters.\")\n",
    "        # logging.debug(f\"Coref Clusters (Indices): {clusters_indices}\")\n",
    "        # logging.debug(f\"Coref Clusters (Strings): {clusters_strings}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FastCoref prediction failed: {e}\")\n",
    "        clusters_indices = [] # Continue without coref if it fails\n",
    "\n",
    "\n",
    "    # 4. Build Initial Coref Map (Character Index -> Representative Mention Text)\n",
    "    logging.info(\"Building initial coreference map...\")\n",
    "    initial_char_coref_map = build_coref_map(stripped_text, clusters_indices)\n",
    "    # logging.debug(f\"Initial Coref Map: {initial_char_coref_map}\")\n",
    "\n",
    "    # 5. Refine Coref Map using NER entities\n",
    "    logging.info(\"Refining coreference map using NER entities...\")\n",
    "    refined_char_coref_map = refine_coref_map_with_entities(initial_char_coref_map, initial_entities)\n",
    "    logging.info(\"Coreference map refined.\")\n",
    "    # logging.debug(f\"Refined Coref Map: {refined_char_coref_map}\")\n",
    "\n",
    "    # 6. Extract Relationships using dependency parse and refined coref map\n",
    "    logging.info(\"Extracting enhanced relationships...\")\n",
    "    relationships = extract_enhanced_relationships(doc, refined_char_coref_map)\n",
    "\n",
    "    # 7. Extract Final Entities using the refined coref map\n",
    "    # This ensures entity nodes in the graph use the resolved names\n",
    "    logging.info(\"Extracting final entities with resolved names...\")\n",
    "    final_entities = extract_final_entities(doc, refined_char_coref_map)\n",
    "\n",
    "    logging.info(\"--- Text Processing Pipeline Complete ---\")\n",
    "    return relationships, final_entities\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 1. Load spaCy Model ---\n",
    "    logging.info(f\"Loading spaCy model: {spacy_model}\")\n",
    "    try:\n",
    "        # Consider disabling components not strictly needed if memory/speed is critical\n",
    "        # nlp = spacy.load(spacy_model, disable=['parser'] if only NER needed elsewhere)\n",
    "        nlp = spacy.load(spacy_model)\n",
    "        logging.info(\"spaCy model loaded successfully.\")\n",
    "    except OSError:\n",
    "        logging.error(f\"spaCy model '{spacy_model}' not found. Please download it: python -m spacy download {spacy_model}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading spaCy model '{spacy_model}': {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "    # --- 2. Initialize FastCoref Model ---\n",
    "    logging.info(\"Initializing FastCoref model...\")\n",
    "    device = 'cpu' # Default to CPU\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            # Test MPS availability properly\n",
    "            torch.tensor([1], device='mps')\n",
    "            device = 'mps'\n",
    "            logging.info(\"Attempting to use MPS (Apple Silicon GPU).\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"MPS device requested but not available or functional ({e}), using CPU.\")\n",
    "            device = 'cpu'\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "        logging.info(\"Attempting to use CUDA GPU.\")\n",
    "\n",
    "    try:\n",
    "        # Pass the loaded spaCy model to FastCoref if it accepts/requires it\n",
    "        # Check FastCoref documentation for the best way to integrate\n",
    "        # Some versions might prefer text only, others benefit from pre-tokenization\n",
    "        fc_model = FCoref(model_name_or_path=fastcoref_model_name, device=device) # Adjust based on FCoref version\n",
    "        # Older usage might have been: FCoref(fastcoref_model_name, nlp=nlp, device=device)\n",
    "        logging.info(f\"Successfully initialized FastCoref model '{fastcoref_model_name}' on {device.upper()}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize FastCoref model on {device.upper()}: {e}\")\n",
    "        # Optionally, try falling back to CPU if GPU failed\n",
    "        if device != 'cpu':\n",
    "            logging.warning(\"Attempting fallback to CPU for FastCoref.\")\n",
    "            try:\n",
    "                device = 'cpu'\n",
    "                fc_model = FCoref(model_name_or_path=fastcoref_model_name, device=device)\n",
    "                logging.info(f\"Successfully initialized FastCoref model '{fastcoref_model_name}' on CPU.\")\n",
    "            except Exception as e2:\n",
    "                 logging.error(f\"Failed to initialize FastCoref model on CPU as fallback: {e2}\")\n",
    "                 exit(1) # Exit if model cannot be loaded\n",
    "        else:\n",
    "             exit(1) # Exit if CPU init failed\n",
    "\n",
    "\n",
    "    # --- 3. Define Input Text ---\n",
    "    text = \"\"\"\n",
    "    Apple Inc., founded by Steve Jobs and Steve Wozniak in 1976, is based in Cupertino, California.\n",
    "    Tim Cook became the CEO of Apple in August 2011 after Jobs resigned. He previously worked at IBM Corp.\n",
    "    Apple produces the popular iPhone smartphone. Its main competitor, Google LLC, makes the Android operating system.\n",
    "    Steve Jobs also co-founded Pixar Animation Studios, which was later acquired by The Walt Disney Company in 2006. Bob Iger leads Disney currently.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 4. Run the Full Pipeline ---\n",
    "    extracted_relationships, extracted_entities = process_text_to_graph_info(\n",
    "        text, nlp, fc_model\n",
    "    )\n",
    "\n",
    "    # --- 5. Display Results ---\n",
    "    print(\"\\n\" + \"=\"*30 + \" FINAL RESULTS \" + \"=\"*30)\n",
    "    print(\"\\nEntities (Resolved Name: Type):\")\n",
    "    if extracted_entities:\n",
    "        # Sort for consistent output\n",
    "        for name in sorted(extracted_entities.keys()):\n",
    "            print(f\"  - {name}: {extracted_entities[name]}\")\n",
    "    else:\n",
    "        print(\"  (No entities found)\")\n",
    "\n",
    "    print(\"\\nRelationships (Subject, Verb, Object, Attributes):\")\n",
    "    if extracted_relationships:\n",
    "        # Sort for consistent output\n",
    "        extracted_relationships.sort()\n",
    "        for rel in extracted_relationships:\n",
    "            subj, verb, obj, attrs = rel\n",
    "            attr_str = f\"| Attributes: {attrs}\" if attrs else \"\"\n",
    "            print(f\"  - ({subj}) --[{verb}]--> ({obj}) {attr_str}\")\n",
    "    else:\n",
    "        print(\"  (No relationships found)\")\n",
    "\n",
    "    print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tolkigpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
